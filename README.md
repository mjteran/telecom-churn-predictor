# Telecom Customer Churn Predictor

Predict customer churn for a telecom company with an end-to-end ML pipeline and an interactive Streamlit dashboard.

---

## ğŸ” Project Overview

Customer churn is a major challenge for telecom companies.  This project:
- Cleans and engineers an extended IBM Telco Churn dataset.  
- Benchmarks several models and selects **Logistic Regression (L1)** for the best recall + interpretability.  
- Serves real-time predictions and insights through a public **Streamlit** app.

---

## ğŸ“Š Dashboard Highlights

| Section | What you can do |
|---------|-----------------|
| **Churn Predictor** | Fill a form â†’ get churn probability & label. |
| **Visualizations** | Explore churn by contract, tenure, city, charges, satisfaction, CLTV, etc. |

---

## â–¶ **Try it now:**

### ğŸŒ Live Demo
<https://mjteran-telecom-churn-predictor.streamlit.app>

### ğŸš€ Run Locally

```bash
# 1 â€“ clone and enter repo
git clone https://github.com/<your-username>/telecom-churn-predictor.git
cd telecom-churn-predictor

# 2 â€“ install dependencies (use a venv if you like)
pip install -r requirements.txt

# 3 â€“ launch Streamlit
streamlit run churn_app.py
```
(Opens <http://localhost:8501> in your browser).

### ğŸ›  Requirements

```text
pandas
scikit-learn
joblib
plotly
streamlit
```

(Automatically installed via `requirements.txt`.)

---

## ğŸ“’ Notebook & Slides

- **EDA + Modeling notebook:** [`notebooks/churn_modeling_exploration.ipynb`](notebook/churn_modeling_exploration.ipynb)  
- **Slide deck:** <https://docs.google.com/presentation/d/15nHl9ydwYCIzIfBo-hEmODlNJwKqVoqxpElqfxdKs_o/edit?usp=sharing>

---

## ğŸ“ Repository Structure

```text
telecom-churn-predictor/
â”œâ”€â”€ churn_app.py                 # Streamlit dashboard
â”œâ”€â”€ requirements.txt             # Python dependencies
â”œâ”€â”€ model/                       # Serialized model + helper objects
â”‚   â”œâ”€â”€ Churn_modeling.py            # Data cleaning, feature engineering, model training
â”‚   â”œâ”€â”€ churn_prediction_lr.pkl
â”‚   â”œâ”€â”€ rob_scaler.pkl
â”‚   â”œâ”€â”€ city_freq_dict.pkl
â”‚   â”œâ”€â”€ city_to_cluster.pkl
â”‚   â””â”€â”€ means_churn_inputs.pkl
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ train.csv                # Raw training split
â”‚   â”œâ”€â”€ test.csv                 # Raw test split
â”‚   â”œâ”€â”€ validation.csv           # Raw validation 
â”‚   â””â”€â”€ df_EDA.csv               # Aggregated data used for dashboard visuals only
â”œâ”€â”€ notebooks/
â”‚   â””â”€â”€ churn_modeling_exploration.ipynb
â””â”€â”€ README.md
```

---

## ğŸ¤– Model Exploration & Selection

| Model  | Accuracy | F1 (churn) | Recall (churn) | Hyper-parameter Tuning |
|--------|----------|------------|----------------|------------------------|
| **Logistic Regression (L1)** | **0.9645** | **0.93** | **0.91** | Grid Search (C, penalty) |
| K-Nearest Neighbors | 0.9304 | 0.86 | 0.79 | Elbow Method to pick optimal *k* |
| Decision Tree | 0.9489 | 0.90 | 0.86 | Cross-validation on `max_depth` |
| Support Vector Machine | 0.9574 | 0.92 | 0.89 | Grid Search on C / kernel |
| Random Forest | 0.9574 | 0.92 | 0.87 | Grid Search (n_estimators, depth, etc.) |
| XGBoost | 0.9617 | 0.93 | 0.90 | Grid Search (learning rate, depth, estimators) |

**Why Logistic Regression?**  
Logistic Regression matches the top F1, gives the highest recall, and avoids overfitting. It offered the best performance with simpler interpretation and faster computation.

*Scaler:* `RobustScaler`â€ƒ|â€ƒ*Extra features:* GeoCluster (K-Means), frequency-encoded city, one-hot/ordinal/boolean encodings.

---

## ğŸ‘©â€ğŸ’» Author

**Maria Jose Teran** â€” Data Analyst & ML Learner  
GitHub â€¢ [@mjteran](https://github.com/mjteran)â€ƒ|â€ƒLinkedIn â€¢ [majoteran92](https://linkedin.com/in/majoteran92)
